{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7c5b42",
   "metadata": {},
   "source": [
    "<img src=\"https://lh3.googleusercontent.com/RF732BF33x9KZfm9GTxSyHSQbmlk4qF9buyVcgu3Mu9ZZAkR7T1QrjBk2j5tuS8nTXFccX6a6mcBTKwGmoFlKWHU18WAYGJgbEfUsiw=w680\" width=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e9cb8",
   "metadata": {},
   "source": [
    "# Catatan Sesi 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f031f3",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d33630",
   "metadata": {},
   "source": [
    "Naive Bayes adalah teknik klasifikasi statistik berdasarkan Bayes Theorem. Ini adalah salah satu supervised learning algorithms yang paling sederhana. Naive Bayes classifier adalah algoritme yang cepat, akurat, dan andal. Naive Bayes classifier memiliki akurasi dan kecepatan tinggi pada kumpulan data besar.\n",
    "\n",
    "Naive Bayes classifier mengasumsikan bahwa efek fitur tertentu dalam kelas tidak bergantung pada fitur lainnya. Misalnya, pemohon pinjaman diloloskan atau tidak tergantung pada pendapatannya, pinjaman sebelumnya dan riwayat transaksi, usia, dan lokasi. Meskipun fitur ini saling bergantung, fitur ini masih dianggap terpisah. Asumsi ini menyederhanakan komputasi, dan itulah mengapa dianggap naive. Asumsi ini disebut class conditional independence.\n",
    "\n",
    "- P(h): The probability of hypothesis h being true (regardless of the data). This is known as the prior probability of h.\n",
    "- P(D): The probability of the data (regardless of the hypothesis). This is known as the prior probability.\n",
    "- P(h|D): The probability of hypothesis h given the data D. This is known as posterior probability.\n",
    "- P(h|D): The probability of data d given that the hypothesis h was true. This is known as posterior probability.\n",
    "How Naive Bayes Classifier Works?\n",
    "Mari kita pahami cara kerja Naive Bayes melalui sebuah contoh. Diberikan contoh kondisi cuaca dan keputusan bermain olah raga. Kita perlu menghitung kemungkinan bermain olahraga. Sekarang, kita perlu mengklasifikasikan apakah pemain akan bermain atau tidak, berdasarkan kondisi cuaca.\n",
    "\n",
    "Naive Bayes classifier menghitung probabilitas suatu peristiwa dalam langkah-langkah berikut:\n",
    "\n",
    "- Step 1: Calculate the prior probability for given class labels.\n",
    "- Step 2: Find likelihood probability with each attribute for each class.\n",
    "- Step 3: Put these value in Bayes Formula and calculate posterior probability.\n",
    "- Step 4: See which class has a higher probability, given the input belongs to the higher probability class.\n",
    "\n",
    "Untuk menyederhanakan perhitungan probabilitas prior dan posterior kita dapat menggunakan dua tables frequency dan likelihood tables. Kedua tabel ini akan membantu kita menghitung probabilitas prior dan posterior. Frequency table berisi kemunculan label untuk semua fitur. Ada dua tabel kemungkinan. Likelihood Table 1 menunjukkan probabilitas sebelumnya dari label dan Likelihood Table 2 menunjukkan probabilitas posterior.\n",
    "\n",
    "Sekarang misalkan kita ingin menghitung probabilitas bermain (Yes/No) saat cuaca mendung (Overcast).\n",
    "\n",
    "Probability of playing:\n",
    "\n",
    "P(Yes | Overcast) = P(Overcast | Yes) P(Yes) / P (Overcast)\n",
    "\n",
    "1. Calculate Prior Probabilities:\n",
    "\n",
    "P(Overcast) = 4/14 = 0.29\n",
    "\n",
    "P(Yes)= 9/14 = 0.64\n",
    "\n",
    "2. Calculate Posterior Probabilities:\n",
    "\n",
    "P(Overcast |Yes) = 4/9 = 0.44\n",
    "\n",
    "3. Put Prior and Posterior probabilities in equation\n",
    "\n",
    "P (Yes | Overcast) = 0.44 * 0.64 / 0.29 = 0.98(Higher)\n",
    "\n",
    "Similarly, you can calculate the probability of not playing:\n",
    "\n",
    "Probability of not playing:\n",
    "\n",
    "P(No | Overcast) = P(Overcast | No) P(No) / P (Overcast)\n",
    "\n",
    "1. Calculate Prior Probabilities:\n",
    "\n",
    "P(Overcast) = 4/14 = 0.29\n",
    "\n",
    "P(No)= 5/14 = 0.36\n",
    "\n",
    "2. Calculate Posterior Probabilities:\n",
    "\n",
    "P(Overcast |No) = 0/9 = 0\n",
    "\n",
    "3. Put Prior and Posterior probabilities in equation\n",
    "P (No | Overcast) = 0 * 0.36 / 0.29 = 0\n",
    "\n",
    "Probabilitas kelas 'Yes' lebih tinggi. Jadi kita bisa menentukan disini jika cuaca mendung maka pemain akan melakukan olah raga.\n",
    "\n",
    "Sekarang misalkan kita ingin menghitung probabilitas bermain saat cuaca mendung (Overcast), dan suhunya sedang (Mild).\n",
    "\n",
    "Probability of playing:\n",
    "\n",
    "P(Play= Yes | Weather=Overcast, Temp=Mild) = P(Weather=Overcast, Temp=Mild | Play= Yes)P(Play=Yes)\n",
    "\n",
    "P(Weather=Overcast, Temp=Mild | Play= Yes)= P(Overcast |Yes) P(Mild |Yes)\n",
    "\n",
    "1. Calculate Prior Probabilities: P(Yes)= 9/14 = 0.64\n",
    "\n",
    "2. Calculate Posterior Probabilities: P(Overcast |Yes) = 4/9 = 0.44 P(Mild |Yes) = 4/9 = 0.44\n",
    "\n",
    "3. Put Posterior probabilities in equation (2) P(Weather=Overcast, Temp=Mild | Play= Yes) = 0.44 * 0.44 = 0.1936(Higher)\n",
    "\n",
    "4. Put Prior and Posterior probabilities in equation (1) P(Play= Yes | Weather=Overcast, Temp=Mild) = 0.1936*0.64 = 0.124\n",
    "\n",
    "Similarly, you can calculate the probability of not playing:\n",
    "Probability of not playing:\n",
    "\n",
    "P(Play= No | Weather=Overcast, Temp=Mild) = P(Weather=Overcast, Temp=Mild | Play= No)P(Play=No)\n",
    "\n",
    "P(Weather=Overcast, Temp=Mild | Play= No)= P(Weather=Overcast |Play=No) P(Temp=Mild | Play=No)\n",
    "\n",
    "1. Calculate Prior Probabilities: P(No)= 5/14 = 0.36\n",
    "\n",
    "2. Calculate Posterior Probabilities: P(Weather=Overcast |Play=No) = 0/9 = 0 P(Temp=Mild | Play=No)=2/5=0.4\n",
    "\n",
    "3. Put posterior probabilities in equation (4) P(Weather=Overcast, Temp=Mild | Play= No) = 0 * 0.4= 0\n",
    "\n",
    "4. Put prior and posterior probabilities in equation (3) P(Play= No | Weather=Overcast, Temp=Mild) = 0*0.36=0\n",
    "\n",
    "Probabilitas kelas 'Yes' lebih tinggi. Jadi bisa dibilang disini kalau cuaca mendung dan suhunya sedang maka pemain akan main olahraga\n",
    "\n",
    "## Naive Bayes Classifier Building in scikit-learn\n",
    "##### Defining Dataset\n",
    "\n",
    "Dalam contoh ini, kita dapat menggunakan kumpulan data dummy dengan tiga kolom weather, temperature, dan play. Dua yang pertama adalah features (weather, temperature) dan yang lainnya adalah label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220f4ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning features and label variables\n",
    "\n",
    "weather = ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy']\n",
    "\n",
    "temp = ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild']\n",
    "\n",
    "play = ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52daf7b4",
   "metadata": {},
   "source": [
    "Encoding Features\n",
    "\n",
    "Pertama, kita perlu mengubah string menjadi angka, misalnya: 'Overcast', 'Rainy', 'Sunny' sebagai 0, 1, 2. Ini dikenal sebagai label encoding. Scikit-learn menyediakan pustaka LabelEncoder untuk mengenkode label dengan nilai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7feeef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Creating LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting string labels into numbers\n",
    "weather_encoded = le.fit_transform(weather)\n",
    "\n",
    "print(weather_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9db4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp:  [1 1 1 2 0 0 0 2 0 2 2 2 1 2]\n",
      "Play:  [0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Converting string labels into numbers\n",
    "temp_encoded = le.fit_transform(temp)\n",
    "label = le.fit_transform(play)\n",
    "\n",
    "print(\"Temp: \", temp_encoded)\n",
    "print(\"Play: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827eb99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp:  [1 1 1 2 0 0 0 2 0 2 2 2 1 2]\n",
      "Play:  [0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Converting string labels into numbers\n",
    "temp_encoded = le.fit_transform(temp)\n",
    "label = le.fit_transform(play)\n",
    "\n",
    "print(\"Temp: \", temp_encoded)\n",
    "print(\"Play: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3800c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1),\n",
       " (2, 1),\n",
       " (0, 1),\n",
       " (1, 2),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (2, 0),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (0, 2),\n",
       " (0, 1),\n",
       " (1, 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining weather and temp into single list of tuples\n",
    "features = list(zip(weather_encoded, temp_encoded))\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb769930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value:  [1]\n"
     ]
    }
   ],
   "source": [
    "# Import Gaussian Naive Bayes Model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(features, label)\n",
    "\n",
    "# Predict Output\n",
    "predicted = model.predict([[0, 2]]) # 0: Overcast, 2: Mild\n",
    "print(\"Predicted Value: \", predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89a667",
   "metadata": {},
   "source": [
    "### Naive Bayes with Multiple Labels\n",
    "Sampai saat ini kita telah mempelajari klasifikasi Naive Bayes dengan label biner. Sekarang kita akan belajar tentang multiple class classification di Naive Bayes. Yang dikenal sebagai multinomial Naive Bayes classification. Misalnya, jika kita ingin mengklasifikasikan artikel berita tentang teknologi, hiburan, politik, atau olahraga.\n",
    "\n",
    "Pada bagian pembuatan model, kita dapat menggunakan dataset wine yang merupakan multi-class classification yang sangat terkenal. Kumpulan data ini adalah hasil dari analisis kimiawi anggur yang ditanam di wilayah yang sama di Italia tetapi berasal dari tiga kultivar berbeda.\n",
    "\n",
    "Set data terdiri dari 13 fitur (alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline) dan type of wine cultivar. Data ini memiliki tiga jenis anggur Class_0, Class_1, dan Class_3. Di sini kita dapat membuat model untuk mengklasifikasikan jenis anggur.\n",
    "\n",
    "Dataset tersedia di pustaka scikit-learn.\n",
    "\n",
    "Loading Data\n",
    "\n",
    "Let's first load the required wine dataset from scikit-learn datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfec155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load dataset\n",
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffed2004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Labels:  ['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "# Print the names of the 13 features\n",
    "print(\"Features: \", wine.feature_names)\n",
    "\n",
    "# Print the label type of wine(class_0, class_1, class_2)\n",
    "print(\"Labels: \", wine.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "132b5682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print data (feature) shape\n",
    "wine.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b57d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      "  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n",
      " [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n",
      "  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n",
      " [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n",
      "  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\n"
     ]
    }
   ],
   "source": [
    "# Print the wine data features (top 5 records)\n",
    "print(wine.data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8455ffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Print the wine labels (0: Class_0, 1: Class_1, 2: Class_2)\n",
    "print(wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f918b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=100) # 70% training & 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "683f5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "494c8dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da8c6d",
   "metadata": {},
   "source": [
    "Decision Tree Classifier\n",
    "Decision tree adalah flowchart-like tree structure di mana internal node mewakili feature(atau attribute), branch mewakili decision rule, dan setiap leaf node mewakili outcome. Node paling atas dalam pohon keputusan dikenal sebagai root node. Root node belajar untuk mempartisi berdasarkan nilai atribut. Root node mempartisi pohon secara rekursif memanggil partisi rekursif. Flowchart-like structure ini membantu kita dalam pengambilan keputusan. Visualisasinya seperti diagram flowchart yang dengan mudah meniru pemikiran tingkat manusia. Itulah mengapa decision trees mudah dipahami dan diinterpretasikan.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4450da49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Labels:  ['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "# Print the names of the 13 features\n",
    "print(\"Features: \", wine.feature_names)\n",
    "\n",
    "# Print the label type of wine(class_0, class_1, class_2)\n",
    "print(\"Labels: \", wine.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f05ecaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print data (feature) shape\n",
    "wine.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "575d528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      "  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n",
      " [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n",
      "  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n",
      " [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n",
      "  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\n"
     ]
    }
   ],
   "source": [
    "# Print the wine data features (top 5 records)\n",
    "print(wine.data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3463306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=100) # 70% training & 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b1abcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "febb482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224afb5",
   "metadata": {},
   "source": [
    "Decision Tree Classifier\n",
    "Decision tree adalah flowchart-like tree structure di mana internal node mewakili feature(atau attribute), branch mewakili decision rule, dan setiap leaf node mewakili outcome. Node paling atas dalam pohon keputusan dikenal sebagai root node. Root node belajar untuk mempartisi berdasarkan nilai atribut. Root node mempartisi pohon secara rekursif memanggil partisi rekursif. Flowchart-like structure ini membantu kita dalam pengambilan keputusan. Visualisasinya seperti diagram flowchart yang dengan mudah meniru pemikiran tingkat manusia. Itulah mengapa decision trees mudah dipahami dan diinterpretasikan.\n",
    "\n",
    "## How Does The Decision Tree Algorithm Work?\n",
    "Ide dasar di balik decision tree algorithm adalah sebagai berikut:\n",
    "\n",
    "Select the best attribute using Attribute Selection Measures (ASM) to split the records.\n",
    "Make that attribute a decision node and breaks the dataset into smaller subset.\n",
    "Starts tree building by repeating this process recursively for each child until one of the condition will match:\n",
    "All the tuples belong to the same attribute value.\n",
    "There are no more remaining attributes.\n",
    "There are no more instances.\n",
    "Decision Tree Classifier Building in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02f673a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "\n",
    "from sklearn import metrics # Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98cc19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"https://raw.githubusercontent.com/ardhiraka/PFDS_sources/master/diabetes.csv\", header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e89aaa20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pregnancies</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>BloodPressure</td>\n",
       "      <td>SkinThickness</td>\n",
       "      <td>Insulin</td>\n",
       "      <td>BMI</td>\n",
       "      <td>DiabetesPedigreeFunction</td>\n",
       "      <td>Age</td>\n",
       "      <td>Outcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pregnant  glucose             bp           skin  insulin   bmi  \\\n",
       "0  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI   \n",
       "1            6      148             72             35        0  33.6   \n",
       "2            1       85             66             29        0  26.6   \n",
       "3            8      183             64              0        0  23.3   \n",
       "4            1       89             66             23       94  28.1   \n",
       "\n",
       "                   pedigree  age    label  \n",
       "0  DiabetesPedigreeFunction  Age  Outcome  \n",
       "1                     0.627   50        1  \n",
       "2                     0.351   31        0  \n",
       "3                     0.672   32        1  \n",
       "4                     0.167   21        0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pima.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d18e63",
   "metadata": {},
   "source": [
    "Feature Selection\n",
    "\n",
    "Di sini, kita perlu membagi kolom yang diberikan menjadi dua jenis variabel dependen (atau variabel target) dan variabel independen (atau variabel fitur).ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7accd5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 769 entries, 0 to 768\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   pregnant  769 non-null    object\n",
      " 1   glucose   769 non-null    object\n",
      " 2   bp        769 non-null    object\n",
      " 3   skin      769 non-null    object\n",
      " 4   insulin   769 non-null    object\n",
      " 5   bmi       769 non-null    object\n",
      " 6   pedigree  769 non-null    object\n",
      " 7   age       769 non-null    object\n",
      " 8   label     769 non-null    object\n",
      "dtypes: object(9)\n",
      "memory usage: 54.2+ KB\n"
     ]
    }
   ],
   "source": [
    "pima.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c518d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numer = ['pregnant', 'insulin', 'bmi', 'age', 'glucose', 'bp', 'pedigree', 'label']\n",
    "\n",
    "for col in numer: # coerce for missing value\n",
    "    pima[col] = pd.to_numeric(pima[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df9bf9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a3fd5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in features and target variable\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age', 'glucose', 'bp', 'pedigree']\n",
    "\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.label # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f644fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training & 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb7d7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree Classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifier\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeea9a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7012987012987013\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ef0dc",
   "metadata": {},
   "source": [
    "Visualizing Decision Tree\n",
    "Kita bisa menggunakan fungsi export_graphviz Scikit-learn untuk menampilkan tree dalam notebook Jupyter. Untuk plotting tree, kita juga perlu menginstall graphviz dan pydotplus.\n",
    "\n",
    "pip install graphviz\n",
    "\n",
    "pip install pydotplus\n",
    "\n",
    "Fungsi export_graphviz mengubah decision tree classifier menjadi dot file dan pydotplus mengonversi file dot ini ke png atau bentuk yang dapat ditampilkan di Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36c1308a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pydotplus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m dot_data \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[0;32m      7\u001b[0m tree\u001b[38;5;241m.\u001b[39mexport_graphviz(clf, \n\u001b[0;32m      8\u001b[0m  out_file\u001b[38;5;241m=\u001b[39mdot_data, \n\u001b[0;32m      9\u001b[0m  class_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;66;03m# the target names.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m  rounded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Whether to round the corners of the boxes.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m  special_characters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 14\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mpydotplus\u001b[49m\u001b[38;5;241m.\u001b[39mgraph_from_dot_data(dot_data\u001b[38;5;241m.\u001b[39mgetvalue()) \n\u001b[0;32m     15\u001b[0m Image(graph\u001b[38;5;241m.\u001b[39mcreate_png())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pydotplus' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn.tree as tree\n",
    "#import pydotplus\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf, \n",
    " out_file=dot_data, \n",
    " class_names=['0','1'], # the target names.\n",
    " feature_names=feature_cols, # the feature names.\n",
    " filled=True, # Whether to fill in the boxes with colours.\n",
    " rounded=True, # Whether to round the corners of the boxes.\n",
    " special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f7679a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1939226669.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [34]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Optimizing Decision Tree Performance\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Optimizing Decision Tree Performance\n",
    "criterion: optional (default=\"gini\") or Choose attribute selection measure: This parameter allows us to use the different-different attribute selection measure. Supported criteria are \"gini\" for the Gini index and \"entropy\" for the information gain.\n",
    "splitter: string, optional (default=\"best\") or Split Strategy: This parameter allows us to choose the split strategy. Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split.\n",
    "max_depth: int or None, optional (default=None) or Maximum Depth of a Tree: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split_samples. The higher value of maximum depth causes overfitting, and a lower value causes underfitting (Source)\n",
    "Dalam Scikit-learn, pengoptimalan decision tree classifier dilakukan hanya dengan pre-pruning. Maximum depth pohon dapat digunakan sebagai variabel kontrol untuk pre-pruning. Dalam contoh berikut, kita dapat memplot decision tree pada data yang sama dengan max_depth = 3. Selain parameter pre-pruning, kita juga dapat mencoba ukuran pemilihan atribut lain seperti entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b049b7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
